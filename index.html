<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8">
  <title>Reconocimiento de Objetos y Rostros</title>
  <style>
    video, canvas {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: auto;
    }
    #log {
      position: absolute;
      top: 10px;
      left: 10px;
      color: white;
      background: rgba(0, 0, 0, 0.6);
      padding: 10px;
      font-family: sans-serif;
      z-index: 10;
    }
  </style>
</head>
<body>

<div id="log">Cargando modelos...</div>
<video id="video" autoplay muted playsinline></video>
<canvas id="canvas"></canvas>

<script defer src="face-api.min.js"></script>
<script type="module">
  import {
    ObjectDetector,
    FilesetResolver
  } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/vision_bundle.mjs";

  const log = document.getElementById('log');
  const video = document.getElementById('video');
  const canvas = document.getElementById('canvas');
  const ctx = canvas.getContext('2d');

  let objectDetector;
  let labeledDescriptors;
  let faceMatcher;
  let running = true;
  let frameCount = 0;
  let lastObjectDetections = [];
  let lastFaceDetections = [];
  let lastFaceMatches = [];

  async function init() {
    log.innerText = 'Cargando modelos...';

    await faceapi.nets.tinyFaceDetector.loadFromUri('https://issitek.github.io/face-and-object-recognition/models');
    await faceapi.nets.faceLandmark68Net.loadFromUri('https://issitek.github.io/face-and-object-recognition/models');
    await faceapi.nets.faceRecognitionNet.loadFromUri('https://issitek.github.io/face-and-object-recognition/models');

    const img = await faceapi.fetchImage('https://issitek.github.io/face-and-object-recognition/img/angel.jpg');
    const detection = await faceapi
      .detectSingleFace(img, new faceapi.TinyFaceDetectorOptions())
      .withFaceLandmarks()
      .withFaceDescriptor();

    if (!detection) {
      log.innerText = 'No se pudo detectar el rostro en la imagen de referencia';
      return;
    }

    labeledDescriptors = [new faceapi.LabeledFaceDescriptors('angel', [detection.descriptor])];
    faceMatcher = new faceapi.FaceMatcher(labeledDescriptors, 0.6);

    log.innerText = 'Modelos de rostro cargados. Cargando modelo de objetos...';

    const vision = await FilesetResolver.forVisionTasks(
      "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm"
    );

    objectDetector = await ObjectDetector.createFromOptions(vision, {
      baseOptions: {
        modelAssetPath: "./models/efficientdet_lite0.tflite",
        delegate: "CPU"
      },
      scoreThreshold: 0.5,
      runningMode: "VIDEO"
    });

    log.innerText = 'Todos los modelos cargados. Iniciando cámara...';
    startCamera();
  }

  function startCamera() {
    navigator.mediaDevices.getUserMedia({ video: true }).then((stream) => {
      video.srcObject = stream;
      video.addEventListener('loadeddata', () => {
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        detectFrame();
      });
    }).catch(err => {
      log.innerText = 'Error al iniciar cámara: ' + err;
    });
  }

  async function detectFrame() {
    if (!running) return;

    ctx.clearRect(0, 0, canvas.width, canvas.height);
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

    const nowInMs = Date.now();

    // EfficientDet cada 2 frames
    if (frameCount % 2 === 0) {
      const result = await objectDetector.detectForVideo(video, nowInMs);
      lastObjectDetections = result.detections || [];
    }

    // FaceAPI cada 3 frames
    if (frameCount % 3 === 0) {
      const faceResults = await faceapi
        .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
        .withFaceLandmarks()
        .withFaceDescriptors();

      const resizedResults = faceapi.resizeResults(faceResults, {
        width: canvas.width,
        height: canvas.height
      });

      lastFaceDetections = resizedResults;
      lastFaceMatches = faceResults.map(fd => faceMatcher.findBestMatch(fd.descriptor));
    }

    drawAll();

    frameCount++;
    requestAnimationFrame(detectFrame);
  }

  function drawAll() {
    log.innerHTML = '';

    // Dibujar objetos
    lastObjectDetections.forEach(det => {
      const box = det.boundingBox;
      const category = det.categories[0];
      const x = box.originX;
      const y = box.originY;
      const width = box.width;
      const height = box.height;

      ctx.strokeStyle = '#00FF00';
      ctx.lineWidth = 2;
      ctx.strokeRect(x, y, width, height);

      ctx.font = "16px sans-serif";
      ctx.fillStyle = 'rgba(0, 0, 0, 0.6)';
      ctx.fillRect(x, y - 20, ctx.measureText(category.categoryName).width + 40, 20);
      ctx.fillStyle = '#FFFFFF';
      ctx.fillText(`${category.categoryName} (${(category.score * 100).toFixed(1)}%)`, x + 5, y - 5);

      log.innerHTML += `${category.categoryName} (${(category.score * 100).toFixed(1)}%)<br>`;
    });

    // Dibujar rostros
    lastFaceDetections.forEach((fd, i) => {
      const match = lastFaceMatches[i];
      const box = fd.detection.box;

      const x = box.x;
      const y = box.y;
      const width = box.width;
      const height = box.height;

      ctx.strokeStyle = 'red';
      ctx.lineWidth = 2;
      ctx.strokeRect(x, y, width, height);

      ctx.font = "16px sans-serif";
      ctx.fillStyle = 'rgba(0, 0, 0, 0.6)';
      ctx.fillRect(x, y - 20, ctx.measureText(match.toString()).width + 10, 20);
      ctx.fillStyle = '#FFFFFF';
      ctx.fillText(match.toString(), x + 5, y - 5);

      log.innerHTML += `Rostro: ${match.toString()}<br>`;
    });
  }

  init();
</script>

</body>
</html>
